{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4367,
     "status": "ok",
     "timestamp": 1615498523400,
     "user": {
      "displayName": "Sam Perochon",
      "photoUrl": "",
      "userId": "02902782722869988029"
     },
     "user_tz": -60
    },
    "id": "Is69xFauBV_r",
    "outputId": "9c7121da-bea8-4ce0-d652-2638d993a2c0"
   },
   "outputs": [],
   "source": [
    "use_colab=True\n",
    "if use_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    #cd /content/drive/MyDrive/MVA/Gaze/GazeCapture/pytorch\n",
    "    ROOT_DATA = '/content/drive/MyDrive/RAKUTEN/data/'\n",
    "    !pip install timm\n",
    "    !pip install transformers\n",
    "    !wget --load-cookies '/content/drive/MyDrive/RAKUTEN/ens.fr_cookies.txt' 'https://challengedata.ens.fr/participants/challenges/59/download/supplementary-files'\n",
    "    !tar -xvf 'supplementary-files'\n",
    "    !cd '/content/drive/MyDrive/RAKUTEN/'\n",
    "else:\n",
    "    #!cd /Users/samperochon/MVA/Deep_Learning/Project/code/GazeCapture/pytorch\n",
    "    ROOT_DATA = '/Users/samperochon/MVA/Sparse Representations/Kaggle/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4367,
     "status": "ok",
     "timestamp": 1615498523400,
     "user": {
      "displayName": "Sam Perochon",
      "photoUrl": "",
      "userId": "02902782722869988029"
     },
     "user_tz": -60
    },
    "id": "Is69xFauBV_r",
    "outputId": "9c7121da-bea8-4ce0-d652-2638d993a2c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content/drive/MyDrive/RAKUTEN/'\n",
      "/Users/samperochon/MVA/Sparse Representations/Kaggle\n"
     ]
    }
   ],
   "source": [
    "!cd '/content/drive/MyDrive/RAKUTEN/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmCFZgx2Z0SZ"
   },
   "source": [
    "# Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": [
     333,
     386,
     428,
     461,
     490,
     524,
     541,
     558,
     567,
     573,
     595
    ],
    "executionInfo": {
     "elapsed": 5910,
     "status": "ok",
     "timestamp": 1615499633318,
     "user": {
      "displayName": "Sam Perochon",
      "photoUrl": "",
      "userId": "02902782722869988029"
     },
     "user_tz": -60
    },
    "id": "3o1wBJ7WZ0SZ"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TextDataset' from 'tools' (/Users/samperochon/MVA/Sparse Representations/Kaggle/tools.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-3eaae9f2b998>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Local import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImgAugTransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageDatasetTEST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageTextDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCustomBertModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TextDataset' from 'tools' (/Users/samperochon/MVA/Sparse Representations/Kaggle/tools.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "from PIL import Image\n",
    "\n",
    "from glob import glob \n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# For the ViT\n",
    "import timm\n",
    "\n",
    "# Local import\n",
    "from tools import (ImgAugTransform, ImageDataset, ImageDatasetTEST, ImageTextDataset, TextDataset, Metric, generate_batch, CustomBertModel)\n",
    "\n",
    "\n",
    "class Experiment():\n",
    "    def __init__(self, # General Properties\n",
    "                         root_data, model_type = 'DenseNet', use_text = False,  epochs=30, start_epoch = 0, batch_size = 256, optimizer = 'AdamW', head_model = 'sequential',\n",
    "                         lr=0.01, lr_decay_epoch = 10, momentum = 0.9, augment = True, weight_decay = 1e-2, dropout = 0.5, relaxation_type = None,\n",
    "                         print_freq = 10, resume_path = '', pos_weight = True, \n",
    "                         split_train_val = 0.8, verbose=True, debug=False,\n",
    "                         # Properties for DenseNet model\n",
    "                         denseNet_attr = {}, \n",
    "                         # Properties for the ViT model\n",
    "                         vit_attr = {}, inception_attr={}, resnet_attr={}):\n",
    "        \n",
    "        # General properties\n",
    "        self.root_data = root_data\n",
    "        self.dir_path = None\n",
    "        self.json_path = None\n",
    "        self.print_freq = print_freq\n",
    "        self.resume_path = resume_path\n",
    "        \n",
    "        # Architectrure properties\n",
    "        self.model_type = model_type if not use_text else 'Bert'\n",
    "        self.use_text = use_text\n",
    "        self.head_model = head_model\n",
    "        self.relaxation_type = relaxation_type\n",
    "    \n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.epochs = epochs\n",
    "        self.start_epoch = start_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.lr = lr\n",
    "        self.lr_decay_epoch = lr_decay_epoch # Decay LR by a factor 10 every lr_decay_epoch epoch\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.dropout = dropout\n",
    "        self.pos_weight = pos_weight\n",
    "        \n",
    "        \n",
    "        # Data parameters\n",
    "        self.augment = augment\n",
    "\n",
    "        self.split_train_val = split_train_val\n",
    "        self.verbose = verbose\n",
    "        self.debug = debug\n",
    "        self.cuda = torch.cuda.is_available()\n",
    "        \n",
    "        # Models attributes\n",
    "        self.denseNet_attr = {'layers' : 100, 'growth' : 12, 'droprate' : 0, 'augment' : True, 'reduce' : 0.5,\n",
    "                               'bottleneck' : True, 'name' : 'DenseNet_BC_100_12'}\n",
    "        self.vit_attr = vit_attr\n",
    "        self.inception_attr = inception_attr\n",
    "        self.resnet_attr = resnet_attr\n",
    "        \n",
    "        # Load the data, dataLoaders, etc...\n",
    "        self.trainLoader, self.valLoader = self._retrieveData()\n",
    "        \n",
    "        # Initialize the model\n",
    "        self.model = self._retrieveModel()\n",
    "\n",
    "        # Create experiment folder\n",
    "        self.initExperimentPath()\n",
    "        \n",
    "        # Criterion definition\n",
    "        if self.pos_weight:\n",
    "            nbr_labels_positive = torch.tensor([25673,71831,34014,33338,2383,8303,21697,28814,8353,12597,25017,10378,24582,10355,23583,12911,3325,51751,14534]) #number of labels\n",
    "            nbr_labels_negative = nbr_labels_positive.sum()*torch.ones(19)-nbr_labels_positive\n",
    "            coeffs = nbr_labels_negative/nbr_labels_positive    #coefficients for each label\n",
    "            if self.cuda:\n",
    "                coeffs = coeffs.cuda()\n",
    "            self.criterion = torch.nn.BCEWithLogitsLoss(pos_weight=coeffs)\n",
    "        else:\n",
    "            self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Define Optimizer\n",
    "        if self.optimizer =='SGD':\n",
    "            self.optimizer = torch.optim.SGD(self.model.parameters(), self.lr,\n",
    "                                            momentum=self.momentum,\n",
    "                                            nesterov=True,\n",
    "                                            weight_decay=self.weight_decay)    \n",
    "        elif self.optimizer =='AdamW':\n",
    "            self.optimizer = torch.optim.AdamW(self.model.parameters(),lr = self.lr, weight_decay = self.weight_decay)\n",
    "\n",
    "        # Save some metrics: _epoch_only is composed of the metric at the end of each epochs (ie has Number_of_epochs values), wereas _all is composed of Number_of_epochs*Number_of_batches values.\n",
    "        self.losses_train_epoch = Metric()\n",
    "        self.losses_train_batch = Metric()\n",
    "        self.f1_train_epoch = Metric()\n",
    "        self.f1_train_batch = Metric()\n",
    "        \n",
    "        self.losses_val_epoch = Metric()\n",
    "        self.losses_val_batch = Metric()\n",
    "        self.f1_val_epoch = Metric()\n",
    "        self.f1_val_batch = Metric()\n",
    "        self.best_f1 = 0     \n",
    "\n",
    "    def _retrieveData(self):\n",
    "        \"\"\"\n",
    "            Return the Train and Test data loader, based on the training data available.\n",
    "        \"\"\"\n",
    "        print('Retrieving the data...')\n",
    "        # Load the training X and Y data\n",
    "        XTrain = pd.read_csv(os.path.join(self.root_data,'X_train_12tkObq.csv'), index_col=0)\n",
    "        YTrain = pd.read_csv(os.path.join(self.root_data,'Y_train_num.csv'), index_col=0)\n",
    "        # Parse the label string target \"['Black','Orange']\" -> ['Black', 'Orange']\n",
    "        #YTrain['color_tags'] = YTrain['color_tags'].apply(lambda x: ast.literal_eval(x))\n",
    "        # Parse the string information: '[1 0 0 0 0 1 0 0 0 0 ]' -> [1, 0, 0, 0, 0, 0, 1, 0, 0, 0 ,0] \n",
    "        YTrain['color_tags_num'] = YTrain['color_tags_num'].apply(lambda x: [float(i) for i in x.strip('[]').split(' ')])    \n",
    "        \n",
    "        \n",
    "\n",
    "        if self.use_text:\n",
    "            \n",
    "            # Load text features \n",
    "            #Xtrain_item_name = torch.load(os.path.join(self.root_data,'data_rakuten/Xtrain_item_name.pt'))\n",
    "            #Xtrain_item_caption = torch.load(os.path.join(self.root_data,'data_rakuten/Xtrain_item_caption.pt'))\n",
    "            Ytrain_label = torch.load(os.path.join(self.root_data,'data_rakuten/Ytrain_label.pt'))\n",
    "            Ytrain_label = Ytrain_label.transpose(1,0)\n",
    "            \n",
    "            train_df_X = XTrain.iloc[:int(Ytrain_label.shape[0]*self.split_train_val)]\n",
    "            train_Y_label = Ytrain_label[:int(Ytrain_label.shape[0]*self.split_train_val)]\n",
    "            \n",
    "            \n",
    "            val_df_X = XTrain.iloc[int(Ytrain_label.shape[0]*self.split_train_val):]\n",
    "            val_Y_label = Ytrain_label[int(Ytrain_label.shape[0]*self.split_train_val):]\n",
    "            \n",
    "            #X_features=torch.cat((Xtrain_item_name,Xtrain_item_caption),0)\n",
    "                                      \n",
    "            #train_df_X = XTrain.iloc[:int(X_features.shape[1]*self.split_train_val)]\n",
    "            #train_df_Y = YTrain.iloc[:int(X_features.shape[1]*self.split_train_val)]\n",
    "                                      \n",
    "            #val_df_X = XTrain.iloc[int(X_features.shape[1]*self.split_train_val):]\n",
    "            #val_df_Y = YTrain.iloc[:int(X_features.shape[1]*self.split_train_val):]\n",
    "                         \n",
    "            # Create datasets\n",
    "            if self.debug:\n",
    "                \n",
    "                trainTextDataset =  TextDataset(train_df_X.iloc[:100], train_Y_label[:100].transpose(1,0))\n",
    "                valTextDataset =  TextDataset(val_df_X.iloc[:100], val_Y_label[:100].transpose(1,0))\n",
    "                \n",
    "                trainTextLoader = torch.utils.data.DataLoader(trainTextDataset, batch_size=self.batch_size,shuffle=True, collate_fn=generate_batch)\n",
    "                valTextLoader = torch.utils.data.DataLoader(valTextDataset, batch_size=self.batch_size,shuffle=False, collate_fn=generate_batch)\n",
    "\n",
    "                #trainDataset = ImageTextDataset(train_df_X, train_df_Y, X_features,  self.root_data, dataset_type='training', augment=self.augment, model_type=self.model_type)\n",
    "                #valDataset = ImageTextDataset(val_df_X, val_df_Y, X_features, self.root_data, dataset_type='validation', augment=self.augment, model_type=self.model_type)   \n",
    "                return(trainTextLoader, valTextLoader)\n",
    "            \n",
    "                #trainDataset = ImageTextDataset(train_df_X.iloc[:100], train_df_Y.iloc[:100], X_features,  self.root_data, dataset_type='training', augment=self.augment, model_type=self.model_type)\n",
    "                #valDataset = ImageTextDataset(val_df_X.iloc[:100], val_df_Y.iloc[:100], X_features,  self.root_data, dataset_type='validation', augment=self.augment, model_type=self.model_type)\n",
    "            else:\n",
    "                trainTextDataset =  TextDataset(train_df_X, train_Y_label.transpose(1,0))\n",
    "                valTextDataset =  TextDataset(val_df_X, val_Y_label.transpose(1,0))\n",
    "                \n",
    "                trainTextLoader = torch.utils.data.DataLoader(trainTextDataset, batch_size=self.batch_size,shuffle=True, collate_fn=generate_batch)\n",
    "                valTextLoader = torch.utils.data.DataLoader(valTextDataset, batch_size=self.batch_size,shuffle=False, collate_fn=generate_batch)\n",
    "\n",
    "                #trainDataset = ImageTextDataset(train_df_X, train_df_Y, X_features,  self.root_data, dataset_type='training', augment=self.augment, model_type=self.model_type)\n",
    "                #valDataset = ImageTextDataset(val_df_X, val_df_Y, X_features, self.root_data, dataset_type='validation', augment=self.augment, model_type=self.model_type)   \n",
    "                return(trainTextLoader, valTextLoader)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "        \n",
    "            # Split tre training data into train and validation sets\n",
    "            train_df_X, train_df_Y, val_df_X, val_df_Y = self._splitTrainVal(XTrain, YTrain, fraction=self.split_train_val)\n",
    "\n",
    "\n",
    "            # Create datasets\n",
    "            if self.debug:\n",
    "                trainImageDataset = ImageDataset(train_df_X.iloc[:100], train_df_Y.iloc[:100], self.root_data, dataset_type='training', augment=self.augment, model_type=self.model_type)\n",
    "                valImageDataset = ImageDataset(val_df_X.iloc[:100], val_df_Y.iloc[:100], self.root_data, dataset_type='validation', augment=self.augment, model_type=self.model_type)\n",
    "            else:\n",
    "                trainImageDataset = ImageDataset(train_df_X, train_df_Y, self.root_data, dataset_type='training', augment=self.augment, model_type=self.model_type)\n",
    "                valImageDataset = ImageDataset(val_df_X, val_df_Y, self.root_data, dataset_type='validation', augment=self.augment, model_type=self.model_type)\n",
    "\n",
    "            # Create Datasets\n",
    "            trainLoader = torch.utils.data.DataLoader(trainImageDataset,batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "            valLoader = torch.utils.data.DataLoader(valImageDataset,batch_size=self.batch_size, shuffle=False, num_workers=2)\n",
    "            \n",
    "            return(trainLoader, valLoader)\n",
    "\n",
    "    \n",
    "    def _retrieveModel(self):\n",
    "        print('Retrieving the model...')\n",
    "        \n",
    "        if self.model_type == 'DenseNet':\n",
    "            # Model creation\n",
    "            model = torch.hub.load('pytorch/vision:v0.6.0', 'densenet121', pretrained=True)\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "                # Replace the last fully-connected layer\n",
    "                # Parameters of newly constructed modules have requires_grad=True by default\n",
    "            model.classifier = nn.Linear(1024, 19) \n",
    "            print('DenseNet model loaded!!!\\nNumber of model parameters to optimize: {}\\n'.format(sum([p.data.nelement() for p in model.parameters() if p.requires_grad])))\n",
    "            \n",
    "        elif self.model_type == 'vit':\n",
    "            model = timm.create_model('vit_large_patch16_224', pretrained=True)\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "                # Replace the last fully-connected layer\n",
    "                # Parameters of newly constructed modules have requires_grad=True by default\n",
    "            model  = self._relaxation(model, type_relax = self.relaxation_type)\n",
    "            if self.head_model == 'sequential':\n",
    "                model.head = nn.Sequential(nn.Linear(1024, 500),\n",
    "                                          nn.BatchNorm1d(num_features=500),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Dropout(self.dropout),\n",
    "                                          nn.Linear(500, 19))\n",
    "            if self.head_model == 'linear':        \n",
    "                model.head = nn.Linear(1024, 19)\n",
    "                                      \n",
    "            print('ViT model loaded!!!\\nNumber of model parameters to optimize: {}\\n'.format(sum([p.data.nelement() for p in model.parameters() if p.requires_grad])))\n",
    "        elif self.model_type == 'inception':\n",
    "            model = torch.hub.load('pytorch/vision:v0.6.0', 'inception_v3', pretrained=True)\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "                # Replace the last fully-connected layer\n",
    "                # Parameters of newly constructed modules have requires_grad=True by default\n",
    "            model.fc = nn.Linear(2048, 19) \n",
    "            print('DenseNet model loaded!!!\\nNumber of model parameters to optimize: {}\\n'.format(sum([p.data.nelement() for p in model.parameters() if p.requires_grad])))\n",
    "        elif self.model_type=='resnet':\n",
    "\n",
    "            model = torchvision.models.resnet152(pretrained=True)\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "                # Replace the last fully-connected layer\n",
    "                # Parameters of newly constructed modules have requires_grad=True by default\n",
    "            model.fc = torch.nn.Linear(model.fc.in_features, 19)\n",
    "            print('DenseNet model loaded!!!\\nNumber of model parameters to optimize: {}\\n'.format(sum([p.data.nelement() for p in model.parameters() if p.requires_grad])))\n",
    "            \n",
    "        elif self.model_type =='Bert':\n",
    "            model  = CustomBertModel()\n",
    "            model.relaxation('soft')\n",
    "            print('DenseBertNet model loaded!!!\\nNumber of model parameters to optimize: {}\\n'.format(sum([p.data.nelement() for p in model.parameters() if p.requires_grad])))\n",
    "        \n",
    "        # Send to cuda if available\n",
    "        if torch.cuda.is_available():\n",
    "            model.to('cuda')\n",
    "\n",
    "        # optionally resume from a checkpoint\n",
    "        if self.resume_path is not '':\n",
    "            if os.path.isfile(self.resume_path):\n",
    "                checkpoint = torch.load(self.resume_path, map_location=torch.device('cpu') if not torch.cuda.is_available() else None)\n",
    "                self.start_epoch = checkpoint['epoch']\n",
    "                self.best_f1 = checkpoint['best_f1']\n",
    "                if self.model_type=='vit':\n",
    "                    model.head.load_state_dict(checkpoint['state_dict'])\n",
    "                elif self.model_type=='DenseNet':\n",
    "                    model.classifier.load_state_dict(checkpoint['state_dict'])\n",
    "                elif self.model_type in ['inception', 'resnet']:\n",
    "                    model.fc.load_state_dict(checkpoint['state_dict'])\n",
    "                print(\"loaded checkpoint '{}' (epoch {}) - Best performance: {}\".format(self.resume_path, checkpoint['epoch'], self.best_f1))\n",
    "            else:\n",
    "                print(\"/!\\ No checkpoint found at '{}'\".format(self.resume_path))\n",
    "        return model\n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            # Divide the LR by a factor 10 every self.lr_decay_epoch epoch\n",
    "            self._adjust_learning_rate(epoch)\n",
    "            \n",
    "            # Train for 1 epoch\n",
    "            loss_final, f1_final = self.train_one_epoch(epoch)\n",
    "            \n",
    "            # Update metrics\n",
    "            self.losses_train_epoch.update(loss_final)\n",
    "            self.f1_train_epoch.update(f1_final)\n",
    "            print('FINAL F1 score at the end of the training of epoch {}: {}'.format(epoch,f1_final))\n",
    "            # Validate on the Validation set\n",
    "            loss_final, f1_final = self.validate(epoch)\n",
    "            \n",
    "            # Update metrics\n",
    "            self.losses_val_epoch.update(loss_final)\n",
    "            self.f1_val_epoch.update(f1_final)\n",
    "            print('FINAL F1 score at the end of the validation of epoch {}: {}'.format(epoch,f1_final))\n",
    "\n",
    "            # Save metrics to the Json file\n",
    "            self.save()\n",
    "            \n",
    "            # remember best f1 and save checkpoint\n",
    "            if self.f1_val_epoch.val > self.best_f1:\n",
    "                worthSaving = True\n",
    "            self._save_checkpoint({'epoch': epoch + 1,\n",
    "                                    'state_dict': self.model.head.state_dict() if ((self.model_type=='vit') and (self.relaxation_type  not in ['soft', 'hard'])) else\n",
    "                                                   self.model.state_dict() if ((self.model_type=='vit') and (self.relaxation_type in ['soft', 'hard'])) else\n",
    "                                                   self.model.classifier.state_dict() if self.model_type=='DenseNet' else\n",
    "                                                   self.model.state_dict() if self.model_type=='Bert' else\n",
    "                                                   self.model.fc.state_dict() if self.model_type in ['inception', 'resnet'] else None,\n",
    "                                    'best_f1': max(self.f1_val_epoch.val, self.best_f1)},\n",
    "                                    is_best =worthSaving,\n",
    "                                    filename = 'checkpoint_{}.pth.tar'.format(epoch))\n",
    "            if self.f1_val_epoch.val > self.best_f1:\n",
    "                self.best_f1 = self.f1_val_epoch.val\n",
    "        return\n",
    "            \n",
    "    def train_one_epoch(self, epoch):\n",
    "        \"\"\"Train for one epoch on the training set\"\"\"\n",
    "        \n",
    "        batch_time = Metric()\n",
    "        losses = Metric()\n",
    "        f1 = Metric()\n",
    "\n",
    "        # switch to train mode\n",
    "        self.model.train()\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (input, target) in enumerate(self.trainLoader):\n",
    "            target = target.cuda()\n",
    "            input = input.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = self.model(input)\n",
    "            if self.model_type == 'inception':\n",
    "                output = output.logits\n",
    "            \n",
    "            # Compute metrics and record\n",
    "            loss = self.criterion(output, target)\n",
    "            \n",
    "            # Update metrics\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            f1.update(f1_score((torch.sigmoid(output).cpu().detach().numpy()>.5).astype(int), target.cpu().detach().numpy(), average='weighted'), input.size(0))\n",
    "            \n",
    "            # Will help to compute the epoch-level F1-score\n",
    "            if i==0:\n",
    "                predictionMemory = (torch.sigmoid(output).cpu().detach().numpy()>.5).astype(int)\n",
    "                targetMemory = target.cpu().detach().numpy()\n",
    "            else:\n",
    "                predictionMemory = np.concatenate((predictionMemory,(torch.sigmoid(output).cpu().detach().numpy()>.5).astype(int)), axis=0)\n",
    "                targetMemory = np.concatenate((targetMemory,target.cpu().detach().numpy()), axis=0)\n",
    "\n",
    "\n",
    "            # compute gradient and do SGD step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % self.print_freq == 0:\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t''Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t''Loss {loss.val:.4f} ({loss.avg:.4f})\\t''F1 {f1.val:.4f} ({f1.avg:.4f})\\t'.format(epoch, i, len(self.trainLoader), batch_time=batch_time,loss=losses, f1=f1))\n",
    "       \n",
    "        self.losses_train_batch.merge(losses)\n",
    "        self.f1_train_batch.merge(f1)\n",
    "\n",
    "        return losses.avg, f1_score(predictionMemory, targetMemory, average='weighted')\n",
    "    \n",
    "    def validate(self, epoch):\n",
    "        \"\"\"Perform validation on the validation set\"\"\"\n",
    "        batch_time = Metric()\n",
    "        losses = Metric()\n",
    "        f1 = Metric()\n",
    "        \n",
    "        # switch to evaluate mode\n",
    "        self.model.eval()\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (input, target) in enumerate(self.valLoader):\n",
    "            target = target.cuda()\n",
    "            input = input.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = self.model(input)\n",
    "            \n",
    "            # Compute metrics and record \n",
    "            loss = self.criterion(output, target)\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            f1.update(f1_score((torch.sigmoid(output).cpu().detach().numpy()>.5).astype(int), target.cpu().detach().numpy(), average='weighted'), input.size(0))\n",
    "            if i==0:\n",
    "                predictionMemory = (torch.sigmoid(output).cpu().detach().numpy()>.5).astype(int)\n",
    "                targetMemory = target.cpu().detach().numpy()\n",
    "            else:\n",
    "                predictionMemory = np.concatenate((predictionMemory,(torch.sigmoid(output).cpu().detach().numpy()>.5).astype(int)), axis=0)\n",
    "                targetMemory = np.concatenate((targetMemory,target.cpu().detach().numpy()), axis=0)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            \n",
    "\n",
    "            if i % self.print_freq == 0:\n",
    "                print('Validation: [{0}/{1}]\\t''Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t''Loss {loss.val:.4f} ({loss.avg:.4f})\\t''F1 {f1.val:.4f} ({f1.avg:.4f})\\t'.format(i, len(self.valLoader), batch_time=batch_time,loss=losses, f1=f1))\n",
    "        \n",
    "\n",
    "        self.f1_val_batch.merge(f1)\n",
    "        self.losses_val_batch.merge(losses)        \n",
    "        return losses.avg, f1_score(predictionMemory, targetMemory, average='weighted')\n",
    "    \n",
    "    \n",
    "    def initExperimentPath(self):\n",
    "        \n",
    "        if not os.path.isdir(os.path.join(self.root_data, 'experiments')):\n",
    "            os.mkdir(os.path.join(self.root_data, 'experiments'))\n",
    "            \n",
    "        if not os.path.isdir(os.path.join(self.root_data, 'experiments', self.model_type)):\n",
    "            os.mkdir(os.path.join(self.root_data, 'experiments', self.model_type))\n",
    "            os.mkdir(os.path.join(self.root_data, 'experiments', self.model_type,'0'))\n",
    "\n",
    "        \n",
    "        experimentNumber = np.max([int(os.path.basename(path)) for path in glob(os.path.join(self.root_data, 'experiments', self.model_type, '*'))])+1\n",
    "        dir_path = os.path.join(self.root_data, 'experiments', self.model_type, str(experimentNumber))\n",
    "        print('Doing experiment {}!'.format(experimentNumber))\n",
    "\n",
    "        os.mkdir(dir_path)\n",
    "        os.mkdir(os.path.join(dir_path, 'models'))\n",
    "        os.mkdir(os.path.join(dir_path, 'figures'))\n",
    "        \n",
    "        json_path = os.path.join(dir_path, 'experiment_log.json')\n",
    "        self.dir_path = dir_path\n",
    "        self.json_path = json_path\n",
    "        \n",
    "        trainLoader = self.trainLoader\n",
    "        valLoader = self.valLoader\n",
    "        model = self.model\n",
    "        \n",
    "        del self.trainLoader, self.valLoader, self.model\n",
    "        with open(json_path, 'w') as outfile:\n",
    "            json.dump(self.__dict__, outfile)\n",
    "            \n",
    "        self.trainLoader, self.valLoader, self.model = trainLoader, valLoader, model\n",
    "        return\n",
    "    \n",
    "    def retrieveExperiment(self, ExperimentNumber=None, extended=False):\n",
    "        \"\"\"\n",
    "            This functions aims at retrieving the best model from all the experiments. \n",
    "            You can either predefince the experiment number and the epoch, or look over all the losses of all experiments and pick the model having the best performances.\n",
    "            \n",
    "            /!\\ Not implemented yet for the Rakuten challenge \n",
    "        \"\"\"\n",
    "        \n",
    "        # Looking for a specified model\n",
    "        if ExperimentNumber is not None:  \n",
    "            self.dir_path = os.path.join(self.root_data, 'experiments',self.model_type,str(ExperimentNumber))\n",
    "            self.resume_path = os.path.join(self.root_data, 'experiments',self.model_type,str(ExperimentNumber), self.model_type+'_model_best.pth.tar')\n",
    "\n",
    "            with open(os.path.join(self.root_data, 'experiments',self.model_type,str(ExperimentNumber),'experiment_log.json')) as jsonFile:\n",
    "                data = json.load(jsonFile)\n",
    "                \n",
    "                # Load attributes\n",
    "                self._load_attributes(data)\n",
    "                \n",
    "                # Load model\n",
    "                self._retrieveModel()\n",
    "            \n",
    "            print('Experiment number {} led to best loss (BCE): {}'.format(ExperimentNumber, self.best_f1))\n",
    "        \n",
    "        if extended:\n",
    "            print('Not implemented yet')\n",
    "            bestExperiment, bestEpoch = self._retrieveBestModel()\n",
    "        return  \n",
    "    \n",
    "    def createSubmissionKaggle(self, path = None):\n",
    "        if path is None:\n",
    "            path = self.dir_path\n",
    "            \n",
    "        test_df_X = pd.read_csv(os.path.join(self.root_data,'X_test_gDTIJPh.csv'), index_col=0)\n",
    "        \n",
    "        # Create Datasets\n",
    "        testImageDataset = ImageDatasetTEST(test_df_X, self.root_data, model_type=self.model_type)\n",
    "        testLoader = torch.utils.data.DataLoader(testImageDataset,batch_size=256, shuffle=False)\n",
    "        \n",
    "        # switch to evaluate mode\n",
    "        self.model.cuda()\n",
    "        self.model.eval()\n",
    "        \n",
    "        for i, (input) in enumerate(testLoader):\n",
    "            input = input.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = self.model(input)\n",
    "            if i==0:\n",
    "                predictionMemory = (torch.sigmoid(output).cpu().detach().numpy()>.5).astype(int)\n",
    "            else:\n",
    "                predictionMemory = np.concatenate((predictionMemory,(torch.sigmoid(output).cpu().detach().numpy()>.5).astype(int)), axis=0)\n",
    "        \n",
    "        idx2color={ 0: \"Beige\",1:\"Black\",2:\"Blue\",3:\"Brown\",4:\"Burgundy\",5:\"Gold\",6:\"Green\",7:\"Grey\",\n",
    "                    8:\"Khaki\",9:\"Multiple Colors\",10:\"Navy\",11:\"Orange\",12:\"Pink\",\n",
    "                   13:\"Purple\",14:\"Red\",15:\"Silver\",16:\"Transparent\",17:\"White\",18:\"Yellow\"}\n",
    "                 \n",
    "        labels = [[idx2color[i]  for i in range(len(item)) if item[i]==1] for item in predictionMemory]\n",
    "        submission_df = pd.DataFrame({'color_tags':labels})\n",
    "        self.submissionKaggle = submission_df\n",
    "        submission_df.to_csv(os.path.join(path,'submissionKaggle.csv'))\n",
    "        return  \n",
    "    \n",
    "    def save(self):\n",
    "        with open(self.json_path) as jsonFile:\n",
    "            data = json.load(jsonFile)\n",
    "            data['losses_train_epoch'] = self.losses_train_epoch.values\n",
    "            data['losses_train_batch'] = self.losses_train_batch.values\n",
    "            data['f1_train_epoch'] = self.f1_train_epoch.values\n",
    "            data['f1_train_batch'] = self.f1_train_batch.values\n",
    "            data['losses_val_epoch'] = self.losses_val_epoch.values\n",
    "            data['losses_val_batch'] = self.losses_val_batch.values\n",
    "            data['f1_val_epoch'] = self.f1_val_epoch.values\n",
    "            data['f1_val_batch'] = self.f1_val_batch.values\n",
    "\n",
    "        with open(self.json_path, 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "      \n",
    "        return\n",
    "    \n",
    "    def _retrieveBestModel(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "        min_loss = np.inf\n",
    "        bestExperiment = None\n",
    "        paths = glob(os.path.join(self.root_data, 'experiments', '*'))\n",
    "        for path in paths:\n",
    "             with open(os.path.join(path,'experiment_log.json')) as jsonFile:\n",
    "                data = json.load(jsonFile)\n",
    "                losses = data['losses']\n",
    "\n",
    "                if len(losses)>0 and np.min(losses)<min_loss:\n",
    "                    min_loss = np.min(losses)\n",
    "                    bestExperiment = os.path.basename(path)\n",
    "                    bestEpoch = np.argmin(losses)\n",
    "        return bestExperiment, bestEpoch\n",
    "    \n",
    "    def _splitTrainVal(self, df_X, df_Y, fraction=0.8):\n",
    "        # splitting dataframe in a particular size \n",
    "        train_df_X = df_X.sample(frac=fraction) \n",
    "        val_df_X = df_X.drop(train_df_X.index)\n",
    "\n",
    "        train_df_Y = df_Y.iloc[train_df_X.index]\n",
    "        val_df_Y = df_Y.drop(train_df_X.index)\n",
    "        return train_df_X.reset_index(drop=True), train_df_Y.reset_index(drop=True), val_df_X.reset_index(drop=True), val_df_Y.reset_index(drop=True)\n",
    "    \n",
    "    def _adjust_learning_rate(self, epoch):\n",
    "        \"\"\"Sets the learning rate to the initial LR decayed by 10 after 150 and 225 epochs\"\"\"\n",
    "        self.lr = self.lr * (0.1 ** (epoch // self.lr_decay_epoch))\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = self.lr\n",
    "                                      \n",
    "    def _relaxation(self, model, type_relax = None):\n",
    "\n",
    "        if type_relax==\"soft\":\n",
    "            for name,param in model.named_parameters():\n",
    "                if name.startswith('blocks.20') or name.startswith('blocks.21') or name.startswith('blocks.22') or name.startswith('blocks.23') :\n",
    "                    param.requires_grad = True\n",
    "        elif type_relax==\"hard\":\n",
    "            for name,param in model.named_parameters():\n",
    "                param.requires_grad = True     \n",
    "        return model    \n",
    "    \n",
    "    def _save_checkpoint(self, state, is_best, filename='checkpoint.pth.tar'):\n",
    "        \"\"\"Saves checkpoint to disk\"\"\"\n",
    "        filename = os.path.join(self.dir_path, 'models', filename)\n",
    "                                             \n",
    "        if self.model_type in ['vit', 'Bert'] and is_best:\n",
    "            torch.save(state, os.path.join(self.dir_path,self.model_type+'_model_best.pth.tar'))                                 \n",
    "        else:\n",
    "            torch.save(state, filename)\n",
    "            if is_best:\n",
    "                shutil.copyfile(filename, os.path.join(self.dir_path,self.model_type+'_model_best.pth.tar'))\n",
    "    \n",
    "    def _load_attributes(self, data):\n",
    "                \n",
    "        # General properties\n",
    "        self.root_data = data['root_data']\n",
    "        self.experiment_path = data['experiment_path']\n",
    "        self.json_path = data['json_path']\n",
    "        self.model_type = data['model_type']\n",
    "        self.epochs = data['epochs']\n",
    "        self.start_epoch = data['start_epoch']\n",
    "        self.batch_size = data['batch_size']\n",
    "        self.lr = data['lr']\n",
    "        self.lr_decay_epoch = data['lr_decay_epoch'] # Decay LR by a factor 10 every lr_decay_epoch epoch\n",
    "        self.momentum = data['momentum']\n",
    "        #self.augment = data['augment'] if 'augment' in data.keys() else False\n",
    "        self.weight_decay = data['weight_decay']\n",
    "        self.print_freq = data['print_freq']\n",
    "        self.resume_path = os.path.join(self.dir_path,self.model_type+'_model_best.pth.tar')\n",
    "        self.split_train_val = data['split_train_val']\n",
    "        self.debug = data['debug']\n",
    "        self.cuda = torch.cuda.is_available()\n",
    "        \n",
    "        #Metrics\n",
    "        self.losses_train_epoch.load(data[\"losses_train_epoch\"])\n",
    "        self.losses_train_batch.load(data[\"losses_train_batch\"])\n",
    "        self.f1_train_epoch.load(data[\"f1_train_epoch\"])\n",
    "        self.f1_train_batch.load(data[\"f1_train_batch\"])\n",
    "        \n",
    "        self.losses_val_epoch.load(data[\"losses_val_epoch\"])\n",
    "        self.losses_val_batch.load(data[\"losses_val_batch\"])\n",
    "        self.f1_val_epoch.load(data[\"f1_val_epoch\"])\n",
    "        self.f1_val_batch.load(data[\"f1_val_batch\"])\n",
    "        \n",
    "        # Models attributes\n",
    "        self.denseNet_attr = data['denseNet_attr']\n",
    "        self.vit_attr = data['vit_attr']\n",
    "        #self.inception_attr = data['inception_attr']\n",
    "        #self.resnet_attr = data['resnet_attr']\n",
    "        return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertExperiment = Experiment(ROOT_DATA, model_type = 'Bert', use_text = True,  epochs=30, start_epoch = 0, batch_size = 64, optimizer = 'AdamW', head_model = 'sequential', print_freq = 100,\n",
    "                           lr=3e-4, lr_decay_epoch = 5, momentum = 0.9, augment = False, weight_decay = 1e-2, dropout = 0.3, relaxation_type = 'soft', debug=False, pos_weight = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "_9jwAxpOAxaW",
    "oD1stwpGAxaX",
    "Rz9fDxGWAxad",
    "m5ApdlEUAxas",
    "mGZMpKzkw_7y"
   ],
   "name": "Development.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
